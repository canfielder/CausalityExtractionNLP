---
title: "Modeling"
author: "Evan Canfield"
date: "11/12/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Purpose

The purpose of this notebook is to recreate the python script *Causality_Classification.py* as a R notebook. 

Several steps in the python script *Causality_Classification.py*, and earlier PDF processing, are executed with R packages available in CRAN. These packages contain similar functions to the Python packages used in the original process, but they may not be exactly equivalent. Successfully recreating the results of *Causality_Classification.py* will help in verifying the R workflow sufficiently recreates the Python workflow.

### A Note on Modifications:

**Errors**

This notebook uses the file *training_data.xlsx* as input. This file was partially manually generated to list of all identified hypotheses sentences in the training set of academic papers. This input also identifies key attributes of each extracted hypothesis. 

**Hypothesis Statement 26** of file **dd96amj.txt** was generating an error when running the python script. The error was caused by a **period** punctuation in the Node 1 entity associated with this hypothesis. During entity extraction, the period was causing the code to not identify a Node 1 entity within hypothesis, and therefore not replace this entity with the term **Node 1**.

Prior to loading the data into this notebook, this period was manually removed from the copy of this input. 


**Scikit Learn Models**
In order to have consistent results, a random seed was set and added to each applicable step.

All other modifications to the *Causality_Classification.py* script were purely formatting driven and have no effect on the execution of the code.


# Import
## Libraries
Direct Library import has been moved to the script R/install.R in order to maintain consistent library management across multiple project actions.
```{r import_libraries}
# Import All Scripts
script_path <- "../R/"
file_paths <- list.files(path = script_path, pattern = ".R", full.names = TRUE)

# Execute All Scripts
for (file in file_paths){
  source(file)
}

# Load Libraries
project_install_packages()
```

## Set Seed and Random States
```{r}
rs <- as.integer(5590)
set.seed(rs)
```


## Python Modules
### Define Python Binary
We first need to point to the python binary we are using. I have had difficultly in the best method to do this. One possible way is to define a Variable Path in the .RProfile for this project. This method has been inconsistent in it's success. Currently, using the **use_python** function from the **Reticulate** package has been successful.
```{r assign_python}
use_python(python = "./../.causalityextractionnlp/bin/python")
```

### Import Python Moducles
```{r import_python_modules}
# General
## Numpy
np <- import("numpy")

# Modeling
## Sci-Kit Learn Model Selection
skl_ms <- import("sklearn.model_selection")

## Sci-Kit Learn Linear Models
skl_lm <- import("sklearn.linear_model")

## Sci-Kit Learn Support Vector Machines 
skl_svm <- import("sklearn.svm")

## Sci-Kit Learn Naive Bayes 
skl_nb <- import("sklearn.naive_bayes")

# NLP
## Gensim
gensim <- import("gensim")
```

## Data
```{r import_data}
train_raw <- read_excel(path = "../data/training_data_node_punct_removed.xlsx", sheet = "training_data")
```


## Set Seed / Random state
```{r set_random}
random_state = integer(5590)
set.seed(random_state)
```

# Pre-Process
Pre-process data before creating Document Term Matrix
```{r pre-processing}
train_processed <- process_data(train_raw)

train_processed
```

## Length Check
```{r}
len_pre <- length(train_raw$file_name)
print(len_pre)

len_post <- length(train_processed$file_name)
print(len_post)
```


# Vectorization
The following steps generates the data features through different emthods of vectorization:

* Bag-of-Words
* Doc2Vec

## Bag-of-Words
The features Document Term Matrix does not include our target variable. Therefore, in addition to generating our features, we also must rejoin our target variable to the newly created DTM. We can also drop our unique row identifier.
```{r dtm_bow}
train_dtm <- gen_dtm_bow(train_processed) %>% 
    left_join(train_processed %>% select(causal_relationship, hyp_id),
            by = c("doc_id" = "hyp_id")) %>% 
  select(causal_relationship, everything()) %>% 
  select(-doc_id)

rownames(train_dtm) <- c()

# Inspect
train_dtm %>% head()
```

## Doc2Vec
### Generate Corpus
We need to generate a Tagged Document object.
```{r}
# Define Tagged Document

TaggedDocument <- gensim$models$doc2vec$TaggedDocument

# Extract Text as List
text <- as.character(train_processed$sentence)

# Initialize
train_corpus <- list()
i = 1

for (hypothesis in text) {
  tokens <- gensim$utils$simple_preprocess(hypothesis)
  train_corpus[[i]] <- TaggedDocument(tokens, as.character(i))
  i = i +1
}
```

### Train Model on Corpus
```{r}
train_corpus <- r_to_py(train_corpus)

model = gensim$models$doc2vec$Doc2Vec(vector_size=50, min_count=2, epochs=40)
model$build_vocab(train_corpus)

model$corpus_count
# Train Model
model$train(train_corpus, total_examples=model$corpus_count, epochs=as.integer(model$epochs))
```

### Transform Text
```{r}
# Extract Test
text <- as.character(train_processed$sentence)

# Initialize
embeddings_d2v <- list()
i = 1

# Generate Embeddings Per Row
for (hypothesis in text) {
  hypothsis_tokens <- str_split(hypothesis, pattern = " ") %>% unlist()
  vector <- model$infer_vector(hypothsis_tokens)
  embeddings_d2v[[i]] <- vector
  i = i +1
}

# Convert List to Dataframe
embeddings_d2v_df <- as.data.frame(embeddings_d2v)

# Transpose Dataframe
embeddings_d2v_df_t <- as.data.frame(t(as.matrix(embeddings_d2v_df)))

# Drop Row Names
rownames(embeddings_d2v_df_t) <- c()

embeddings_d2v_df_t %>% head()

```


# Modeling
## Initialize Models
We will be evaluating the same model types for two different methods of vectorization, Bag-of-Words and Doc2Vec. Before we get to fitting these models to these datasets, we will initialize.
```{r}
# Logistic Regression
lgreg <- skl_lm$LogisticRegression
lgreg_m <- lgreg(C = 1e5, random_state = rs)

# Naive Bayes
nb_m <- skl_nb$MultinomialNB()

# SVM
svc <- skl_svm$SVC
svc_m <- svc(kernel = 'linear', 
             random_state = rs)
```

## Doc2Vec
### Train / Test Split
Convert Doc2Vec features to Python object..
```{r}
features_d2v_py <- r_to_py(embeddings_d2v_df_t)
```

In order to duplicate modeling results in Python, we will use the Python train/test split function, with the same random state, to ensure the training and test sets are consistent. 
```{r }
# Initialize Train/Test Split
train_test_split <- skl_ms$train_test_split

# Split Data
data_split = train_test_split(features_d2v_py, 
                              target_py, 
                              test_size=0.25, 
                              random_state=rs)

# Extract Datasets
X_tr <- data_split[[1]]
X_te <- data_split[[2]]
y_tr <- data_split[[3]]
y_te <- data_split[[4]]

# Unravel Targets
y_tr <- np$ravel(y_tr)
y_te <- np$ravel(y_te)
```

### Logistic Regression
#### Train and Predict
```{r}
# Train 
lgreg_m_d2v <- lgreg_m$fit(X = X_tr, y = y_tr)

# Predict
y_pred_lgreg <- lgreg_m_d2v$predict(X_te)
```

#### Evaluate
```{r}
# Convert to Factors for Caret Package
y_pred_lgreg <- as.factor(y_pred_lgreg)
y_te <- as.factor(y_te)

confusionMatrix(data = y_pred_lgreg, 
                reference = y_te, 
                mode = "prec_recall")
```

### Support Vector Machines
#### Train and Predict
```{r}
# Train 
svc_m_d2v <- svc_m$fit(X = X_tr, y = y_tr)

# Predict
y_pred_svc <- svc_m_d2v$predict(X_te)
```

#### Evaluate
```{r}
# Convert to Factors for Caret Package
y_pred_svc <- as.factor(y_pred_svc)
y_te <- as.factor(y_te)

confusionMatrix(data = y_pred_svc, 
                reference = y_te, 
                mode = "prec_recall")
```

## Bag-of-Words
### Train / Test Split
First, we split the data our target and feature sets. As we will be using Python modules for processing this data, we also need to convert our R dataframes to Python objects.
```{r}
target <- train_dtm %>% select(causal_relationship)
features_bow <- train_dtm %>% select(-causal_relationship)

target_py <- r_to_py(target)
features_bow_py <- r_to_py(features_bow)
```

In order to duplicate modeling results in Python, we will use the Python train/test split function, with the same random state, to ensure the training and test sets are consistent. 
```{r }
# Initialize Train/Test Split
train_test_split <- skl_ms$train_test_split

# Split Data
data_split = train_test_split(features_bow_py, 
                              target_py, 
                              test_size=0.25, 
                              random_state=rs)

# Extract Datasets
X_tr <- data_split[[1]]
X_te <- data_split[[2]]
y_tr <- data_split[[3]]
y_te <- data_split[[4]]

# Unravel Targets
y_tr <- np$ravel(y_tr)
y_te <- np$ravel(y_te)
length(y_te)
```

### Logistic Regression
#### Train and Predict
```{r}
# Train 
lgreg_m_bow <- lgreg_m$fit(X = X_tr, y = y_tr)

# Predict
y_pred_lgreg <- lgreg_m_bow$predict(X_te)
```

#### Evaluate
```{r}
# Convert to Factors for Caret Package
y_pred_lgreg <- as.factor(y_pred_lgreg)
y_te <- as.factor(y_te)

confusionMatrix(data = y_pred_lgreg, 
                reference = y_te, 
                mode = "prec_recall")
```

### Support Vector Machines
#### Train and Predict
```{r}
# Train 
svc_m_bow <- svc_m$fit(X = X_tr, y = y_tr)

# Predict
y_pred_svc <- svc_m_bow$predict(X_te)
```

#### Evaluate
```{r}
# Convert to Factors for Caret Package
y_pred_svc <- as.factor(y_pred_svc)
y_te <- as.factor(y_te)

confusionMatrix(data = y_pred_svc, 
                reference = y_te, 
                mode = "prec_recall")
```

### Naive Bayes
#### Train and Predict
```{r}
# Train 
nb_m_bow <- nb_m$fit(X = X_tr, y = y_tr)

# Predict
y_pred_nb <- nb_m_bow$predict(X_te)
```

#### Evaluate
```{r}
# Convert to Factors for Caret Package
y_pred_nb <- as.factor(y_pred_nb)
y_te <- as.factor(y_te)

confusionMatrix(data = y_pred_nb, 
                reference = y_te, 
                mode = "prec_recall")
```
